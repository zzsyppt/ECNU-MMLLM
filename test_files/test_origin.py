import transformers
import torch
model_id = "/home/pod/shared-nvme/ECNU_MMLLM/llama"

pipeline = transformers.pipeline(
    "text-generation",
    model=model_id,
    model_kwargs={"torch_dtype": torch.bfloat16},
    device="cuda",
)

sys_prompt = (
    "任务：根据用户提供的心率数据，根据其数值与起伏波动等，分析用户的心情状态、心情颜色、能量星级、放松度星级。\n"
    "请返回一个只包含以下指标的 JSON 对象，不需要任何其他文字：\n"
    "1. \"心情状态\": 用一个词描述用户的心情状态。\n"
    "2. \"心情颜色\": 能代表用户心情状态的颜色的编码。\n"
    "3. \"总能量星级\": 用户的整体能量评级，介于1到5。\n"
    "4. \"放松度星级\": 用户的放松程度评级，介于1到5。\n"
    "5. \"压力指数星级\": 用户的压力水平评级，介于1到5。\n"
    "6. \"情绪稳定性星级\": 用户的情绪稳定性评级，介于1到5。\n"
    "7. \"心理弹性星级\": 用户的心理弹性评级，介于1到5。\n"
    "8. \"身心和谐度星级\": 用户的身心和谐程度评级，介于1到5。\n"
    "9. \"综合得分\": 用户的综合得分，介于1到5。\n"
    "10. \"建议\": 根据用户的心情状态，给出一些建议以促进良好的生活。\n"
    "请严格遵循格式，并仅返回 JSON 对象，不要添加任何其他内容。"
    "我再次强调：请不要在 JSON 周围添加其他信息，比如 '这是 JSON 对象'。"
)

series = '76,76,76,74,72,72,72,72,72,78,84,84,82,82,80,78,78,78,78,76,76,76,76,76,70,70,72,72,76,78,78,76,76,76,70,68,68,68,68,72,72,70,70,68,68,68,68,68,68,70,70,70,70,70,70,70,68,68,68,70,72,74,74,74,74,72,72,72,70,68,68,68,68,70,72,72,74,68,68,66,66,66,72,74,76,76,76,74,74,74,74,72,72,70,68,70,70,70,70,70,70,70,70,70,70,70,66,68,68,68,68,70,70,68,68,66,66,70,70,70,74,77,77,78,80,80,80,80,80,74,74,70,70,70,70,68,68,70,70,71,71,77,80,80,80,78,78,76,76,72,72,72,72,70,68,70,72,76,80,80,80,80,80,80,80,78,78,74,74,71,68,67,65,65,65,65,69,69,76,76,74,74,74,72,72,72,70,68,68,66,66,66,68,68,70,70,72,72,74,74,74,74,74,72,70,70,66,66,66,66,66,66,66,70,72,74,74,72,72,72,72,74,74,72,70,70,70,72,72,72,72,72,70,72,70,72,72,74,78,80,80,80,80,78,76,76,76,76,76,72,72,66,66,66,66,63,63,61,61,61,61,68,68,66,64,63,61,61,61,61,61,63,63,63,63,68,70,70,66,63,61,61,61,61,61,61,61,66,66,66,66,63,63,63,59,59,59,59,59,59,59,59,59,59,59,61,61,61,61,63,63,63,63,64,66,66,64,63,61,61,61,59,59,59,59,59,61,61,64,64,64,64,61,59,59,59,59,59,59,59,59,57,57,57,57,63,68,68,70,70,70,71,74,74,77,78,78,76,72,72,72,64,64,64,64,66,68,70,70,72,68,68,68,68,63,64,66,66,66,66,70,70,70,70,63,59,59,59,63,63,64,64,64,63,63,63,63,63,63,61,61,61,61,61,63,63,63,61,61,61,61,61,61,63,64,64,66,64,63,63,61,59,59,59,59,59,59,59,59,59,59,59,59,61,61,61,61,63,63,70,70,74,74,72,72,68,72,72,72,70,70,70,61,59,59,59,59,61,63,63,63,63,61,61,61,61,61,61,61,61,61,61,66,63,63,63,63,63,65,65,68,74,74,80,77,77,71,68,66,66,64,64,59,59,59,59,61,61,61,63,63,64,61,61,61,63,63,64,64,66,66,64,64,64,61,60,60,60,60,60,57,57,59,59,59,59,59,59,59,59,59,61,61,61,61,61,61,64,64,64,63,63,61,57,57,63,66,63,63,60,60,60,60,60,60,59,59,63,64,64,64,63,61,61,59,59,59,59,59,59,59,57,57,59,59,59,59,59,59,59,61,59,59,59,61,61,63,63,63,66,66,64,63,61,61,57,57,57,57,57,57,57,57,57,57,59,59,59,59,59,59,59,59,59,59,63,63,61,59,61,63,63,64,63,63,61,59,59,59,59,59,59,59,59,59,59,59,59,59,59,59,59,59,59,59,59,61,61,61,61,63,63,63,63,61,61,61,61,61,63,63,63,63,63,61,61,61,61,61,61,61,59,59,59,57,57,59,59,59,61,61,61,61,61,61,61,61,61,61,59,61,61,61,61,61,61,61,61,59,59,59,59,59,59,59,59,57,57,59,59,61,61,61,59,59,59,57,59,62,63,60,60,60,71,74,77,80,80,80,80,74,66,64,63,61,61,61,61,61,61,63,63,63,63,63,70,74,74,68,68,63,63,61,61,59,59,59,59,64,66,66,66,66,66,70,69,70,70,70,70,70,70,70,70,60,60,60,60,60,60,57,57,57,59,61,63,66,70,72,75,75,75,69,69,68,65,62,62,60,60,60,60,60,60,60,60,61,63,63,64,63,61,59,57,57,57,57,57,57,57,57,57,57,57,57,57,63,63,64,64,61,57,57,55,55,57,59,59,61,61,61,59,59,59,59,59,59,61,61,68,68,64,63,61,59,59,59,57,57,57,57,59,59,57,57,59,61,61,63,63,63,59,59,59,59,59,59,59,59,59,59,59,59,57,57,59,61,63,63,63,63,64,64,72,72,78,78,78,78,78,72,72,64,63,61,61,61,63,63,64,64,64,64,66,66,66,63,64,68,68,72,72,72,72,72,63,60,60,60,60,60,63,64,66,66,68,68,68,68,68,68,70,70,72,74,76,76,74,74,74,74,74,60,60,60,60,61,64,68,70,70,70,63,59,59,57,57,57,57,57,57,57,57,59,59,59,59,59,59,59,66,66,66,60,60,60,60,60,61,68,68,68,68,80,80,82,82,84,84,84,80,80,80,74,72,72,70,68,68,63,63,61,61,61,61,64,64,64,61,61,61,63,63,63,61,61,61,61,66,70,72,69,69,69,66,66,64,64,64,64,66,66,66,65,67,67,67,63,63,63,64,64,65,64,64,64,78,78,78,78,76,76,76,76,76,76,78,78,78,76,76,76,74,72,72,70,70,66,63,61,61,61,61,61,61,61,63,63,63,68,74,76,76,78,76,78,78,78,78,74,72,74,74,74,74,72,70,70,70,63,63,63,61,61,61,63,63,64,64,64,61,61,61,61,61,61,59,59,59,64,64,68,70,70,70,74,74,74,74,72,72,72,74,74,72,72,72,72,66,64,64,64,63,63,63,63,63,63,63,64,66,66,72,72,72,72,74,74,74,74,74,74,74,70,70,70,61,61,57,57,57,57,57,59,59,59,55,51,51,60,62,65,65,70,70,72,74,74,76,76,74,74,72,66,63,60,60,60,61,61,61,66,68,68,68,66,63,61,61,61,61,61,61,61,72,76,76'

messages = [
    {"role": "system", "content": sys_prompt},
    {"role": "user", "content": series},
]

prompt = pipeline.tokenizer.apply_chat_template(
		messages, 
		tokenize=False, 
		add_generation_prompt=True
)

terminators = [
    pipeline.tokenizer.eos_token_id,
    pipeline.tokenizer.convert_tokens_to_ids("<|eot_id|>")
]

outputs = pipeline(
    prompt,
    max_new_tokens=256,
    eos_token_id=terminators,
    do_sample=True,
    temperature=0.6,
    top_p=0.9,
)
print(outputs[0]["generated_text"][len(prompt):])